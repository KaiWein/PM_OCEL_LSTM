{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import (prep, folding, inbu)\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras.layers import LSTM, Input\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# import session_info\n",
    "# session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place order': 'A', 'pick item': 'B', 'confirm order': 'C', 'item out of stock': 'D', 'reorder item': 'E', 'pay order': 'F', 'create package': 'G', 'send package': 'H', 'failed delivery': 'I', 'package delivered': 'J', 'payment reminder': 'K'}\n",
      "{'Marco Pegoraro': 'a', 'Gyunam Park': 'b', 'Majid Rafiei': 'c', 'Junxiong Gao': 'd', 'Seran Uysal': 'e', 'Christina Rensinghof': 'f', 'Wil van der Aalst': 'g', 'Christine Dobbert': 'h', 'Luis Santos': 'i', 'Kefang Ding': 'j', 'Mohammadreza Fani Sani': 'k', 'Tobias Brockhoff': 'l', 'Anahita Farhang Ghahfarokhi': 'm', 'Mahnaz Qafari': 'n', 'Claudia Graf': 'o', 'Mahsa Bafrani': 'p', 'Lisa Mannel': 'q'}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "filename = \"running-example\"\n",
    "flattent_by = 'Orders'\n",
    "ocel, act_dict, cust_dict = prep.prepare_flat_ocel(filename, flatten_on= flattent_by)\n",
    "print(act_dict)\n",
    "print(cust_dict)\n",
    "drops_col_order = [\"weight\", \"price\", \"Event_ID\", 'Products']\n",
    "enriched_log, single_log = prep.gen_enriched_single_plus_csv(OCEL = ocel,flatted_by = 'Orders',csvname = 'orders_complete', drops_col= drops_col_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 103436.38622985176\n",
      "divisor2: 604261.1036767652\n",
      "divisorTR: 1248093.355995932\n"
     ]
    }
   ],
   "source": [
    "enriched_log =prep.gen_features(enriched_log)\n",
    "divisor = np.mean(enriched_log['Time_Diff'])  # average time between events\n",
    "divisor2 = np.mean(enriched_log['Time_Since_Start'])  # average time between current and first events\n",
    "divisorTR = np.mean(enriched_log['Remaining_Time'])  # average time instance remaining\n",
    "print(f\"divisor: {divisor}\")\n",
    "print(f\"divisor2: {divisor2}\")\n",
    "print(f\"divisorTR: {divisorTR}\")\n",
    "enr_train, enr_test = folding.folding_train_test(enriched_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_feat = list(filter(lambda k: k.startswith('Act_') and not k.startswith('Next_Act_'), enr_train.columns))\n",
    "target_act_feat = list(filter(lambda k: k.startswith('Next_Act_') and not k.startswith('Act_'), enr_train.columns))\n",
    "act_feat.remove('Act_!')\n",
    "cust_feat = list(filter(lambda k: 'Cust_' in k, enr_train.columns))\n",
    "time_feat = ['Time_Diff', 'Time_Since_Start', 'Time_Since_Midnight','Weekday', 'Remaining_Time']\n",
    "other_features = ['Amount_Items','In_Package']\n",
    "feature_select = act_feat + cust_feat + time_feat + other_features\n",
    "print(f\"Length of act_feat: {len(act_feat)}, Length of cust_feat: {len(cust_feat)}\")\n",
    "\n",
    "## define dimensions of inputs\n",
    "traces, max_trace_length = prep.gen_traces_and_maxlength_of_trace(enriched_log)\n",
    "target_act_length = len(target_act_feat)\n",
    "number_of_train_cases = len(enr_train)\n",
    "num_of_features = len(feature_select)\n",
    "print(f\"Number of train cases: {number_of_train_cases}, Max trace length: {max_trace_length}, Number of features: {num_of_features}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = np.zeros((number_of_train_cases, max_trace_length, num_of_features), dtype=np.float32)\n",
    "\n",
    "\n",
    "y_train_a, y_train_t, y_train_tr = inbu.generating_inputs(OCEL=enr_train,taf=target_act_feat, divisor_next=divisor,divisor_remaining=divisorTR)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"This matches the desired shape (number_of_train_cases, max_trace_length, num_of_features): {(number_of_train_cases, max_trace_length, num_of_features)} => {X_train.shape ==(number_of_train_cases, max_trace_length, num_of_features)}\")\n",
    "print(f\"Shape of y_train_a: {y_train_a.shape}, this matches the desired shape (number_of_train_cases, target_act_length): {(number_of_train_cases, target_act_length)} => {y_train_a.shape ==(number_of_train_cases, target_act_length)}\")\n",
    "print(f\"Shape of y_train_t: {y_train_t.shape}, this matches the desired shape (number_of_train_cases, ): {(number_of_train_cases, )} => {y_train_t.shape ==(number_of_train_cases, )}\")\n",
    "print(f\"Shape of y_train_tr: {y_train_tr.shape}, this matches the desired shape (number_of_train_cases, ): {(number_of_train_cases, )} => {y_train_tr.shape ==(number_of_train_cases, )}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((number_of_train_cases, max_trace_length, num_of_features), dtype=np.float32)\n",
    "for i,row in enr_train.itertuples():\n",
    "    \n",
    "    for i in \n",
    "    leftpad = max_trace_length - len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "filename = \"running-example\"\n",
    "flattent_by = 'Orders'\n",
    "ocel, act_dict, cust_dict = prep.prepare_flat_ocel(filename, flatten_on= flattent_by)\n",
    "print(act_dict)\n",
    "print(cust_dict)\n",
    "drops_col_order = [\"weight\", \"price\", \"Event_ID\", 'Products']\n",
    "enriched_log, single_log = prep.gen_enriched_single_plus_csv(OCEL = ocel,flatted_by = 'Orders',csvname = 'orders_complete', drops_col= drops_col_order)\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "Ptimeseqs = []\n",
    "Ptimeseqs2 = []\n",
    "Ptimeseqs3 = []\n",
    "Ptimeseqs4 = []\n",
    "nb_itemseqs = []\n",
    "\n",
    "Ptimes = []\n",
    "Ptimes2 = []\n",
    "Ptimes3 = []\n",
    "Ptimes4 = []\n",
    "nb_items = []\n",
    "ascii_offset = 64\n",
    "Pcasestarttime = None\n",
    "Plasteventtime = None\n",
    "\n",
    "for index, row in enriched_log.iterrows():\n",
    "    t = row['Timestamp']\n",
    "    nb = row['Amount_Items']\n",
    "    if row['Case_ID'] != lastcase:\n",
    "        Stime = t\n",
    "        Pcasestarttime = t\n",
    "        Plasteventtime = t\n",
    "        lastcase = row['Case_ID']\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            Ptimeseqs.append(Ptimes)\n",
    "            Ptimeseqs2.append(Ptimes2)\n",
    "            Ptimeseqs3.append(Ptimes3)\n",
    "            Ptimeseqs4.append(Ptimes4)\n",
    "            nb_itemseqs.append(nb_items)\n",
    "\n",
    "        line = ''\n",
    "        Ptimes = []\n",
    "        Ptimes2 = []\n",
    "        Ptimes3 = []\n",
    "        Ptimes4 = []\n",
    "        nb_items = []\n",
    "\n",
    "    line += row['Activity'] #unichr(int(row['Activity']) + ascii_offset)\n",
    "    Ptimesincelastevent = t - Plasteventtime\n",
    "    Ptimesincecasestart = t - Pcasestarttime\n",
    "    midnight = t.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = t - midnight\n",
    "    Ptimediff = 86400 * Ptimesincelastevent.days + Ptimesincelastevent.seconds\n",
    "    Ptimediff2 = 86400 * Ptimesincecasestart.days + Ptimesincecasestart.seconds\n",
    "    Ptimediff3 = timesincemidnight.seconds\n",
    "    Ptimediff4 = t.weekday()\n",
    "    Ptimes.append(Ptimediff)\n",
    "    Ptimes2.append(Ptimediff2)\n",
    "    Ptimes3.append(Ptimediff3)\n",
    "    Ptimes4.append(Ptimediff4)\n",
    "    nb_items.append(nb)\n",
    "    Plasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# Add last case\n",
    "lines.append(line)\n",
    "Ptimeseqs.append(Ptimes)\n",
    "Ptimeseqs2.append(Ptimes2)\n",
    "Ptimeseqs3.append(Ptimes3)\n",
    "Ptimeseqs4.append(Ptimes4)\n",
    "nb_itemseqs.append(nb_items)\n",
    "\n",
    "PtimeseqsF = []\n",
    "for seq in Ptimeseqs2:\n",
    "    PtimeseqsF.append([seq[-1] - t for t in seq])\n",
    "\n",
    "\n",
    "\n",
    "divisor = np.mean([item for sublist in Ptimeseqs for item in sublist])  # average time between events\n",
    "divisor2 = np.mean([item for sublist in Ptimeseqs2 for item in sublist])  # average time between current and first events\n",
    "divisorTR = np.mean([item for sublist in PtimeseqsF for item in sublist])  # average time instance remaining\n",
    "print(f\"divisor: {divisor}\")\n",
    "print(f\"divisor2: {divisor2}\")\n",
    "print(f\"divisorTR: {divisorTR}\")\n",
    "print(f'numoof lines {len(lines)}')\n",
    "print(len(nb_itemseqs))\n",
    "### folding the lists \n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5, lines_t6 = folding.folding_based_arrays(lines,Ptimeseqs,Ptimeseqs2,Ptimeseqs3,Ptimeseqs4,nb_itemseqs,PtimeseqsF,seeded = 42)\n",
    "lines = list(map(lambda x: x + '!', lines))  # put delimiter symbol\n",
    "maxlen = max(map(lambda x: len(x), lines))  # find maximum line size\n",
    "\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "# chars = map(lambda x: set(x),lines)\n",
    "chars = list(set().union(*map(lambda x: set(x), lines)))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "sentences_t5 = []\n",
    "sentences_t6 = []\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "next_chars_t5 = []\n",
    "next_chars_t6 = []\n",
    "\n",
    "\n",
    "for line, line_t, line_t2, line_t3, line_t4, line_t5, line_t6 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5, lines_t6):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        # we add iteratively, first symbol of the line, then two first, three...\n",
    "\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        sentences_t5.append(line_t5[0:i])\n",
    "        sentences_t6.append(line_t6[0:i])\n",
    "        next_chars.append(line[i])\n",
    "        if i == len(line) - 1:  # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "            next_chars_t5.append(0)\n",
    "            next_chars_t6.append(0)\n",
    "\n",
    "\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "            next_chars_t5.append(line_t5[i])\n",
    "            next_chars_t6.append(line_t6[i])\n",
    "\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "num_features = len(chars) + 6\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "y_tr = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen - len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    next_tr = next_chars_t6[i] # wrong in original code was t5\n",
    "\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "    sentence_t5 = sentences_t5[i]\n",
    "\n",
    "    for t, char in enumerate(sentence):\n",
    "        for c in chars:\n",
    "            if c == char:  # this will encode present events to the right places\n",
    "                X[i, t + leftpad, char_indices[c]] = 1\n",
    "        X[i, t + leftpad, len(chars)] = t + 1\n",
    "        X[i, t + leftpad, len(chars) + 1] = sentence_t[t] / divisor\n",
    "        X[i, t + leftpad, len(chars) + 2] = sentence_t2[t] / divisor2\n",
    "        X[i, t + leftpad, len(chars) + 3] = sentence_t3[t] / 86400\n",
    "        X[i, t + leftpad, len(chars) + 4] = sentence_t4[t] / 7\n",
    "        X[i, t + leftpad, len(chars) + 5] = sentence_t5[t]              # number of objects\n",
    "\n",
    "    for c in target_chars:\n",
    "        if c == next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1 - softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness / (len(target_chars) - 1)\n",
    "    y_t[i] = next_t / divisor\n",
    "    y_tr[i] = next_tr / divisorTR\n",
    "\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chars_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y_a: {y_a.shape}\")\n",
    "print(f\"Shape of y_t: {y_t.shape}\")\n",
    "print(f\"Shape of y_tr: {y_tr.shape}\")\n",
    "# Reshape the 3D array into 2D\n",
    "reshaped_data = X.reshape((-1, X.shape[-1]))\n",
    "\n",
    "# Convert reshaped ndarray to DataFrame\n",
    "df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "display(df[77:85])#.drop_duplicates())\n",
    "display(df[40:43])#.drop_duplicates())\n",
    "# display(df[-43:])#.drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(\n",
    "    main_input)  # the shared layer\n",
    "b1 = tf.keras.layers.BatchNormalization()(l1)\n",
    "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in activity prediction\n",
    "b2_1 = tf.keras.layers.BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time prediction\n",
    "b2_2 = tf.keras.layers.BatchNormalization()(l2_2)\n",
    "l2_3 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time remaining prediction\n",
    "b2_3 = tf.keras.layers.BatchNormalization()(l2_3)\n",
    "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(\n",
    "    b2_1)\n",
    "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "timeR_output = Dense(1, kernel_initializer='glorot_uniform', name='timeR_output')(b2_3)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output, timeR_output])\n",
    "\n",
    "model.compile(loss={'act_output': 'categorical_crossentropy', 'time_output': 'mae', 'timeR_output': 'mae'}, optimizer='nadam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "model_checkpoint = ModelCheckpoint('./output_files/models/model_'+filename+'_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss',\n",
    "                                   verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, verbose=0, mode='auto', min_delta=0.0001,\n",
    "                               cooldown=0, min_lr=0)\n",
    "\n",
    "history = model.fit(X, {'act_output': y_a, 'time_output': y_t, 'timeR_output': y_tr}, validation_split=0.2, verbose=2,\n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)\n",
    "# list all data in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
