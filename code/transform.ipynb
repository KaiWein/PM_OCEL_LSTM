{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place order': 8, 'pick item': 7, 'confirm order': 0, 'item out of stock': 3, 'reorder item': 9, 'pay order': 5, 'create package': 1, 'send package': 10, 'failed delivery': 2, 'package delivered': 4, 'payment reminder': 6}\n",
      "{'Marco Pegoraro': 12, 'Gyunam Park': 4, 'Majid Rafiei': 11, 'Junxiong Gao': 5, 'Seran Uysal': 14, 'Christina Rensinghof': 1, 'Wil van der Aalst': 16, 'Christine Dobbert': 2, 'Luis Santos': 8, 'Kefang Ding': 6, 'Mohammadreza Fani Sani': 13, 'Tobias Brockhoff': 15, 'Anahita Farhang Ghahfarokhi': 0, 'Mahnaz Qafari': 9, 'Claudia Graf': 3, 'Mahsa Bafrani': 10, 'Lisa Mannel': 7}\n"
     ]
    }
   ],
   "source": [
    "from functions import transform_functions as traf\n",
    "from functions import preprossesing as prep\n",
    "from functions import folding\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from six import unichr\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras.layers import LSTM, Input\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import csv\n",
    "from numpy.compat import unicode\n",
    "\n",
    "\n",
    "filename = \"running-example\"\n",
    "ocel, act_dict, cust_dict = traf.prepare_ocel(filename)\n",
    "col = [\"Items\", \"Products\", \"Orders\",]\n",
    "# for c in col:\n",
    "#     print(c)\n",
    "#     print(np.unique([len(t) for t in ocel[c]]))\n",
    "print(act_dict)\n",
    "print(cust_dict)\n",
    "# display(ocel.head())\n",
    "drops_col_order = [\"weight\", \"price\", \"Event_ID\", 'Products']\n",
    "ocel_orders = traf.flatten(ocel, 'Orders')\n",
    "enriched_log, single_log = traf.gen_flatted_comp_csv(\n",
    "    ocel_orders, 'Orders', drops_col_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enriched_log['Case_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 103436.38622985176\n",
      "divisor2: 604261.1036767652\n",
      "divisor2: 1248093.355995932\n",
      "total chars: 11, target chars: 12\n",
      "{0: '@', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J'}\n",
      "nb sequences: 21620\n",
      "Vectorization...\n",
      "num features: 16\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "Ptimeseqs = []\n",
    "Ptimeseqs2 = []\n",
    "Ptimeseqs3 = []\n",
    "Ptimeseqs4 = []\n",
    "\n",
    "Ptimes = []\n",
    "Ptimes2 = []\n",
    "Ptimes3 = []\n",
    "Ptimes4 = []\n",
    "ascii_offset = 64\n",
    "numlines = 0\n",
    "Pcasestarttime = None\n",
    "Plasteventtime = None\n",
    "\n",
    "for index, row in enriched_log.iterrows():\n",
    "    t = row['Timestamp']\n",
    "\n",
    "    if row['Case_ID'] != lastcase:\n",
    "        Stime = t\n",
    "        Pcasestarttime = t\n",
    "        Plasteventtime = t\n",
    "        lastcase = row['Case_ID']\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            Ptimeseqs.append(Ptimes)\n",
    "            Ptimeseqs2.append(Ptimes2)\n",
    "            Ptimeseqs3.append(Ptimes3)\n",
    "            Ptimeseqs4.append(Ptimes4)\n",
    "\n",
    "        line = ''\n",
    "        Ptimes = []\n",
    "        Ptimes2 = []\n",
    "        Ptimes3 = []\n",
    "        Ptimes4 = []\n",
    "\n",
    "        numlines += 1\n",
    "\n",
    "    line += unichr(int(row['Activity']) + ascii_offset)\n",
    "    Ptimesincelastevent = t - Plasteventtime\n",
    "    Ptimesincecasestart = t - Pcasestarttime\n",
    "    midnight = t.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = t - midnight\n",
    "    Ptimediff = 86400 * Ptimesincelastevent.days + Ptimesincelastevent.seconds\n",
    "    Ptimediff2 = 86400 * Ptimesincecasestart.days + Ptimesincecasestart.seconds\n",
    "    Ptimediff3 = timesincemidnight.seconds\n",
    "    Ptimediff4 = t.weekday()\n",
    "    Ptimes.append(Ptimediff)\n",
    "    Ptimes2.append(Ptimediff2)\n",
    "    Ptimes3.append(Ptimediff3)\n",
    "    Ptimes4.append(Ptimediff4)\n",
    "\n",
    "    Plasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# Add last case\n",
    "lines.append(line)\n",
    "Ptimeseqs.append(Ptimes)\n",
    "Ptimeseqs2.append(Ptimes2)\n",
    "Ptimeseqs3.append(Ptimes3)\n",
    "Ptimeseqs4.append(Ptimes4)\n",
    "\n",
    "PtimeseqsF = []\n",
    "for seq in Ptimeseqs2:\n",
    "    PtimeseqsF.append([seq[-1] - t for t in seq])\n",
    "\n",
    "numlines += 1\n",
    "\n",
    "\n",
    "divisor = np.mean([item for sublist in Ptimeseqs for item in sublist])  # average time between events\n",
    "divisor2 = np.mean([item for sublist in Ptimeseqs2 for item in sublist])  # average time between current and first events\n",
    "divisorTR = np.mean([item for sublist in PtimeseqsF for item in sublist])  # average time instance remaining\n",
    "print(f\"divisor: {divisor}\")\n",
    "print(f\"divisor2: {divisor2}\")\n",
    "print(f\"divisorTR: {divisorTR}\")\n",
    "\n",
    "### folding the lists \n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5 = folding.folding_this(lines,Ptimeseqs,Ptimeseqs2,Ptimeseqs3,Ptimeseqs4,PtimeseqsF,numlines,seeded = 42)\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x + '!', lines))  # put delimiter symbol\n",
    "maxlen = max(map(lambda x: len(x), lines))  # find maximum line size\n",
    "\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "# chars = map(lambda x: set(x),lines)\n",
    "chars = list(set().union(*map(lambda x: set(x), lines)))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "sentences_t5 = []\n",
    "\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "next_chars_t5 = []\n",
    "\n",
    "for line, line_t, line_t2, line_t3, line_t4, line_t5 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        # we add iteratively, first symbol of the line, then two first, three...\n",
    "\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        sentences_t5.append(line_t5[0:i])\n",
    "\n",
    "        next_chars.append(line[i])\n",
    "        if i == len(line) - 1:  # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "            next_chars_t5.append(0)\n",
    "\n",
    "\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "            next_chars_t5.append(line_t5[i])\n",
    "\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "num_features = len(chars) + 5\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "y_tr = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen - len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    next_tr = next_chars_t5[i]\n",
    "\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "\n",
    "    for t, char in enumerate(sentence):\n",
    "        multiset_abstraction = Counter(sentence[:t + 1])\n",
    "        for c in chars:\n",
    "            if c == char:  # this will encode present events to the right places\n",
    "                X[i, t + leftpad, char_indices[c]] = 1\n",
    "        X[i, t + leftpad, len(chars)] = t + 1\n",
    "        X[i, t + leftpad, len(chars) + 1] = sentence_t[t] / divisor\n",
    "        X[i, t + leftpad, len(chars) + 2] = sentence_t2[t] / divisor2\n",
    "        X[i, t + leftpad, len(chars) + 3] = sentence_t3[t] / 86400\n",
    "        X[i, t + leftpad, len(chars) + 4] = sentence_t4[t] / 7\n",
    "\n",
    "    for c in target_chars:\n",
    "        if c == next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1 - softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness / (len(target_chars) - 1)\n",
    "    y_t[i] = next_t / divisor\n",
    "    y_tr[i] = next_tr / divisorTR\n",
    "\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21620\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(\n",
    "    main_input)  # the shared layer\n",
    "b1 = tf.keras.layers.BatchNormalization()(l1)\n",
    "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in activity prediction\n",
    "b2_1 = tf.keras.layers.BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time prediction\n",
    "b2_2 = tf.keras.layers.BatchNormalization()(l2_2)\n",
    "l2_3 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time remaining prediction\n",
    "b2_3 = tf.keras.layers.BatchNormalization()(l2_3)\n",
    "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(\n",
    "    b2_1)\n",
    "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "timeR_output = Dense(1, kernel_initializer='glorot_uniform', name='timeR_output')(b2_3)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output, timeR_output])\n",
    "\n",
    "model.compile(loss={'act_output': 'categorical_crossentropy', 'time_output': 'mae', 'timeR_output': 'mae'}, optimizer='nadam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "model_checkpoint = ModelCheckpoint('output_files/models/model_'+eventlog+'_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss',\n",
    "                                   verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, verbose=0, mode='auto', min_delta=0.0001,\n",
    "                               cooldown=0, min_lr=0)\n",
    "\n",
    "history = model.fit(X, {'act_output': y_a, 'time_output': y_t, 'timeR_output': y_tr}, validation_split=0.2, verbose=2,\n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)\n",
    "# list all data in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### comparing if product and item match but is to hard to handle them\n",
    "\n",
    "t1 = [len(t) for t in ocel['Items']]\n",
    "t2 = [len(t) for t in ocel['Products']]\n",
    "t3 = [a!=b for a, b in zip(t1, t2)]\n",
    "print(ocel.size)\n",
    "print(sum(t3))\n",
    "print(sum(t3)/ocel.size)\n",
    "print(len(t3) - sum(t3))\n",
    "# display(ocel[t3])\n",
    "# display(ocel[t3].explode('Products').groupby(['Products']).size().sort_values())\n",
    "oceltemp = ocel[t3].copy()\n",
    "t1 = [len(t) for t in oceltemp['Items']]\n",
    "t2 = [len(t) for t in oceltemp['Products']]\n",
    "t3 = [a!=b for a, b in zip(t1, t2)]\n",
    "print(sum(t3))\n",
    "df_agg = ocel.copy()\n",
    "df_agg.Items = df_agg.Items.apply(len)\n",
    "df_agg.Orders = df_agg.Orders.apply(len)\n",
    "df_agg.Products = df_agg.Products.apply(len)\n",
    "df_agg = df_agg.groupby('Packages').sum(numeric_only=True)\n",
    "display(df_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
