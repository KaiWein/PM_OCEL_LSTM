{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import preprossesing as prep\n",
    "from functions import folding\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from six import unichr\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras.layers import LSTM, Input\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import csv\n",
    "from numpy.compat import unicode\n",
    "\n",
    "\n",
    "filename = \"running-example\"\n",
    "ocel, act_dict, cust_dict = prep.prepare_ocel(filename)\n",
    "col = [\"Items\", \"Products\", \"Orders\",]\n",
    "# for c in col:\n",
    "#     print(c)\n",
    "#     print(np.unique([len(t) for t in ocel[c]]))\n",
    "print(act_dict)\n",
    "print(cust_dict)\n",
    "# print(pack_dict)\n",
    "# display(ocel.head())\n",
    "drops_col_order = [\"weight\", \"price\", \"Event_ID\", 'Products']\n",
    "ocel_orders = prep.flatten(ocel, 'Orders')\n",
    "enriched_log, single_log = prep.gen_flatted_comp_csv(\n",
    "    ocel_orders, 'Orders', drops_col_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_index = ocel_orders.groupby('Packages')['Orders'] \\\n",
    "                            .apply(lambda x: len(x.unique())) \\\n",
    "                            .loc[lambda x: x > 1] \\\n",
    "                            .index\n",
    "filtered_index\n",
    "ocel_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "Ptimeseqs = []\n",
    "Ptimeseqs2 = []\n",
    "Ptimeseqs3 = []\n",
    "Ptimeseqs4 = []\n",
    "nb_itemseqs = []\n",
    "\n",
    "Ptimes = []\n",
    "Ptimes2 = []\n",
    "Ptimes3 = []\n",
    "Ptimes4 = []\n",
    "nb_items = []\n",
    "ascii_offset = 64\n",
    "Pcasestarttime = None\n",
    "Plasteventtime = None\n",
    "\n",
    "for index, row in enriched_log.iterrows():\n",
    "    t = row['Timestamp']\n",
    "    nb = row['amount_of_items']\n",
    "    if row['Case_ID'] != lastcase:\n",
    "        Stime = t\n",
    "        Pcasestarttime = t\n",
    "        Plasteventtime = t\n",
    "        lastcase = row['Case_ID']\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            Ptimeseqs.append(Ptimes)\n",
    "            Ptimeseqs2.append(Ptimes2)\n",
    "            Ptimeseqs3.append(Ptimes3)\n",
    "            Ptimeseqs4.append(Ptimes4)\n",
    "            nb_itemseqs.append(nb_items)\n",
    "\n",
    "        line = ''\n",
    "        Ptimes = []\n",
    "        Ptimes2 = []\n",
    "        Ptimes3 = []\n",
    "        Ptimes4 = []\n",
    "        nb_items = []\n",
    "\n",
    "    line += unichr(int(row['Activity']) + ascii_offset)\n",
    "    Ptimesincelastevent = t - Plasteventtime\n",
    "    Ptimesincecasestart = t - Pcasestarttime\n",
    "    midnight = t.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = t - midnight\n",
    "    Ptimediff = 86400 * Ptimesincelastevent.days + Ptimesincelastevent.seconds\n",
    "    Ptimediff2 = 86400 * Ptimesincecasestart.days + Ptimesincecasestart.seconds\n",
    "    Ptimediff3 = timesincemidnight.seconds\n",
    "    Ptimediff4 = t.weekday()\n",
    "    Ptimes.append(Ptimediff)\n",
    "    Ptimes2.append(Ptimediff2)\n",
    "    Ptimes3.append(Ptimediff3)\n",
    "    Ptimes4.append(Ptimediff4)\n",
    "    nb_items.append(nb)\n",
    "    Plasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# Add last case\n",
    "lines.append(line)\n",
    "Ptimeseqs.append(Ptimes)\n",
    "Ptimeseqs2.append(Ptimes2)\n",
    "Ptimeseqs3.append(Ptimes3)\n",
    "Ptimeseqs4.append(Ptimes4)\n",
    "nb_itemseqs.append(nb_items)\n",
    "\n",
    "PtimeseqsF = []\n",
    "for seq in Ptimeseqs2:\n",
    "    PtimeseqsF.append([seq[-1] - t for t in seq])\n",
    "\n",
    "\n",
    "\n",
    "divisor = np.mean([item for sublist in Ptimeseqs for item in sublist])  # average time between events\n",
    "divisor2 = np.mean([item for sublist in Ptimeseqs2 for item in sublist])  # average time between current and first events\n",
    "divisorTR = np.mean([item for sublist in PtimeseqsF for item in sublist])  # average time instance remaining\n",
    "print(f\"divisor: {divisor}\")\n",
    "print(f\"divisor2: {divisor2}\")\n",
    "print(f\"divisorTR: {divisorTR}\")\n",
    "print(f'numoof lines {len(lines)}')\n",
    "print(len(nb_itemseqs))\n",
    "### folding the lists \n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5, lines_t6 = folding.folding_this(lines,Ptimeseqs,Ptimeseqs2,Ptimeseqs3,Ptimeseqs4,nb_itemseqs,PtimeseqsF,seeded = 42)\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x + '!', lines))  # put delimiter symbol\n",
    "maxlen = max(map(lambda x: len(x), lines))  # find maximum line size\n",
    "\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "# chars = map(lambda x: set(x),lines)\n",
    "chars = list(set().union(*map(lambda x: set(x), lines)))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)\n",
    "\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "sentences_t5 = []\n",
    "sentences_t6 = []\n",
    "\n",
    "\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "next_chars_t5 = []\n",
    "next_chars_t6 = []\n",
    "\n",
    "\n",
    "for line, line_t, line_t2, line_t3, line_t4, line_t5, line_t6 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4, lines_t5, lines_t6):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        # we add iteratively, first symbol of the line, then two first, three...\n",
    "\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        sentences_t5.append(line_t5[0:i])\n",
    "        sentences_t6.append(line_t6[0:i])\n",
    "        next_chars.append(line[i])\n",
    "        if i == len(line) - 1:  # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "            next_chars_t5.append(0)\n",
    "            next_chars_t6.append(0)\n",
    "\n",
    "\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "            next_chars_t5.append(line_t5[i])\n",
    "            next_chars_t6.append(line_t6[i])\n",
    "\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "num_features = len(chars) + 6\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "y_tr = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen - len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    next_tr = next_chars_t6[i] # wrong in original code\n",
    "\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "    sentence_t5 = sentences_t5[i]\n",
    "\n",
    "    for t, char in enumerate(sentence):\n",
    "        for c in chars:\n",
    "            if c == char:  # this will encode present events to the right places\n",
    "                X[i, t + leftpad, char_indices[c]] = 1\n",
    "        X[i, t + leftpad, len(chars)] = t + 1\n",
    "        X[i, t + leftpad, len(chars) + 1] = sentence_t[t] / divisor\n",
    "        X[i, t + leftpad, len(chars) + 2] = sentence_t2[t] / divisor2\n",
    "        X[i, t + leftpad, len(chars) + 3] = sentence_t3[t] / 86400\n",
    "        X[i, t + leftpad, len(chars) + 4] = sentence_t4[t] / 7\n",
    "\n",
    "    for c in target_chars:\n",
    "        if c == next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1 - softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness / (len(target_chars) - 1)\n",
    "    y_t[i] = next_t / divisor\n",
    "    y_tr[i] = next_tr / divisorTR\n",
    "\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y_a: {y_a.shape}\")\n",
    "print(f\"Shape of y_t: {y_t.shape}\")\n",
    "print(f\"Shape of y_tr: {y_tr.shape}\")\n",
    "# Reshape the 3D array into 2D\n",
    "reshaped_data = X.reshape((-1, X.shape[-1]))\n",
    "\n",
    "# Convert reshaped ndarray to DataFrame\n",
    "df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "display(df[77:85])#.drop_duplicates())\n",
    "display(df[40:42])#.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(\n",
    "    main_input)  # the shared layer\n",
    "b1 = tf.keras.layers.BatchNormalization()(l1)\n",
    "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in activity prediction\n",
    "b2_1 = tf.keras.layers.BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time prediction\n",
    "b2_2 = tf.keras.layers.BatchNormalization()(l2_2)\n",
    "l2_3 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(\n",
    "    b1)  # the layer specialized in time remaining prediction\n",
    "b2_3 = tf.keras.layers.BatchNormalization()(l2_3)\n",
    "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(\n",
    "    b2_1)\n",
    "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "timeR_output = Dense(1, kernel_initializer='glorot_uniform', name='timeR_output')(b2_3)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output, timeR_output])\n",
    "\n",
    "model.compile(loss={'act_output': 'categorical_crossentropy', 'time_output': 'mae', 'timeR_output': 'mae'}, optimizer='nadam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "model_checkpoint = ModelCheckpoint('./output_files/models/model_'+filename+'_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss',\n",
    "                                   verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, verbose=0, mode='auto', min_delta=0.0001,\n",
    "                               cooldown=0, min_lr=0)\n",
    "\n",
    "history = model.fit(X, {'act_output': y_a, 'time_output': y_t, 'timeR_output': y_tr}, validation_split=0.2, verbose=2,\n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)\n",
    "# list all data in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
